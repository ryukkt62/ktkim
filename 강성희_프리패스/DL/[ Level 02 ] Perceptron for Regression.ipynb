{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important;}</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 온라인 1:1 과외반 <Level 2>에 오신 것을 환영합니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview\n",
    "\n",
    "Level 1에서 인공지능과 딥러닝에 대하여 알아보았습니다. 이번 Level 2에서는 딥러닝 알고리즘의 가장 기본이 되는 인공신경망(Artificial Neural Network, ANN), 그 중에서도 가장 기초가 되는 Perceptron(퍼셉트론)에 대한 개념을 배우고 이 알고리즘을 학습하는 방식 중 크게 세 가지, `1) Random Search`, `2) h-step Search`, `3) Gradient Descent`을 배우며, 이 중에 어떤 것이 가장 좋고 어떤 것을 선택해야하는지를 배웁니다. 그리고 이 알고리즘의 핵심이 되는 몇몇 알고리즘들(Gradient Descent, MSE Loss 등)에 대해서 살펴봅니다. 마지막에는 배운 내용을 바탕으로 과제를 풀게 됩니다. 과제는 보스턴의 부동산 관련 정보를 데이터로 정리한 Boston housing dataset을 활용해 부동산의 집값을 예측하는 알고리즘을 퍼셉트론으로 구현하는 것입니다.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "인공신경망(Artificial Neural Network, ANN)은 기본적으로 인간의 신경망(Neural Network, NN)의 작동 원리를 따라한 알고리즘입니다. 인간의 신경망에서서 신경세포 하나를 **뉴런(Neuron)**이라하며 뉴런이 동작하는 방식은 다음과 같습니다. 각 뉴런에 가해지는 자극이 임계치 이상이 되면 뉴런이 활성화되며 다음 뉴런으로 자극을 전달합니다.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Qu5VPA_YFrQt-OytneRfdfopKsFJcBYc\" >\n",
    "\n",
    "\n",
    "인공신경망에선 이 뉴런을 **퍼셉트론(Perceptron)** 또는 **노드(Node)**라 부릅니다. 퍼셉트론이 동작하는 방식은 다음과 같습니다. 노드(Node)는 다양한 입력값(Input, x)을 받으며 가중치(Weight, w)와 입력값을 곱한 것을 모두 합한 값이 [활성함수(Activation Function)](https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/)에 의해 판단되는데, 그 값이 임계치(보통 0)보다 크면 노드가 활성화되고 결과값(Output, y)을 출력하며 다음 노드에 결과값을 전달하고 노드가 활성화되지 않으면 정해진 결과값을 전달합니다. 결과값은 활성함수에 따라 다릅니다.\n",
    "\n",
    "\n",
    "<img src = \"https://drive.google.com/uc?export=view&id=1ihJO6LqbSFTDXtSDipd7rvwejuL9It-D\" />\n",
    "\n",
    "출처 : https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d\n",
    "\n",
    "\n",
    "**single-layer perceptron(단층 퍼셉트론)**은 하나의 노드 또는 퍼셉트론으로 이루어진 가장 기초적인 인공신경망이며 1957년에 프랑크 로젠블라트(Frank Rosenblatt)에 의해 고안되었습니다. 단층 신경망은 결과값이 0 또는 1이 될 수 있어 가장 간단한 형태의 [선형분류기(Linear Classifier)](http://compneurosci.com/wiki/images/c/c0/Linear_Classification.pdf)으로도 볼 수 있으며 단층 신경망은 [XOR](http://www.birc.co.kr/2018/01/22/xor-%EB%AC%B8%EC%A0%9C%EC%99%80-neural-network/) 연산이 불가능하지만, **다층 퍼셉트론(multi-layer perceptron)**으로는 [XOR](http://www.birc.co.kr/2018/01/22/xor-%EB%AC%B8%EC%A0%9C%EC%99%80-neural-network/) 연산이 가능하고 이 점은 인공신경망의 매우 큰 장점입니다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼 단층 신경망을 포함한 딥러닝 알고리즘에서 중요한 것은 가중치(Weight)를 잘 조절해서 알고리즘의 성능을 높이는 것인데 딥러닝도 기계학습(Machine Learning)이기 때문에 가중치를 **학습**해 나가며 최적의 가중치 찾게 됩니다. \n",
    "\n",
    "딥러닝 알고리즘의 거의 모든 가중치 학습 방법은 큰 틀에서 다음과 같습니다.\n",
    "\n",
    "1. weight 초기화한다. 대부분 랜덤으로 weight를 초기화한다.\n",
    "2. 학습 데이터를 딥러닝 알고리즘에 입력하며 결과를 도출한다.\n",
    "3. 결과가 잘 개선되도록(성능이 좋아지도록) w를 개선한다.\n",
    "4. 2~3을 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 퍼셉트론(Perceptron)에 대하여 알아보았으니, 이제 이 알고리즘을 학습하는 방법, 즉 가장 성능이 좋은 가중치를 구하는 방법에 대하여 코드로 실습하며 알아보도록 하겠습니다.\n",
    "\n",
    "우선 퍼셉트론 알고리즘으로 풀 수 있는 가장 간결한 문제(Case)를 정의하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1 - `y = 0.3 * x1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 문제(Case 1)는 한 개의 **feature(x1)**로 이루어진 데이터셋(입력값)과 학습할 **가중치(w1)**, 그리고 정답지인 **Label(y)**로 이루어져 있습니다. `y = 0.3 * x1`이며 `w1 * x1`값이 __y__에 최대한 가까워 지도록 **w1**을 학습할 것입니다. 즉, 0.3에 최대한 근접한 최적의 w1을 구하는 문제입니다.\n",
    "\n",
    "x1은 -1 ~ +1 사이의 랜덤한 값(데이터) 100개로 이루어진 데이터셋으로 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습을 위해 numpy 라이브러리를 불러옵니다.\n",
    "# numpy는 고성능의 수치계산을 위해 사용하는 기본적이면서도 중요한 패키지입니다. 다양하고 빠른 연산을 지원합니다.\n",
    "# 특히 딥러닝(Deep Learning)에서는, numpy를 이용한 '연산의 벡터화' 개념이 중요합니다.\n",
    "# 따라서 뒤의 <Level 3>에서 numpy를 더욱 자세히 배우게 됩니다.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.88899321, -0.07180365, -0.61440994,  0.16378975,  0.24016842,\n",
       "        0.36844803, -0.79312489,  0.49095153, -0.43604186,  0.50684917])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy의 사용하여 랜덤한 값을 만들 것입니다.\n",
    "# numpy의 random(np.random)은 랜덤 값을 생성하는 기능들을 포함하고 있습니다.\n",
    "# 코드를 실행할 때마다 랜덤으로 값이 만들어지기 때문에 결과를 고정하기 위해 seed를 고정합니다.\n",
    "# 고정한 seed라 영향을 미치는 범위는 같은 셀(cell)에서 실행되는 random입니다.\n",
    "# seed 숫자는 임의적으로 37로 고정하겠습니다. \n",
    "np.random.seed(37)\n",
    "\n",
    "# np.random.uniform()은 최솟값(low), 최댓값(high), 개수(size)를 입력받습니다.\n",
    "# 최솟값과 최댓값 사이에서 지정한 개수만큼 랜덤하게 실수(Real Number)를 추출합니다.\n",
    "x1 = np.random.uniform(low=-1.0, high=1.0, size=100)\n",
    "\n",
    "# x1의 shape를 확인합니다.\n",
    "# 100개가 잘 뽑혔다면 (100, 1) 또는 (100, )이 출력됩니다.\n",
    "print(x1.shape)\n",
    "\n",
    "# x1의 첫 10개 값을 확인합니다.\n",
    "x1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.26669796, -0.0215411 , -0.18432298,  0.04913693,  0.07205053,\n",
       "        0.11053441, -0.23793747,  0.14728546, -0.13081256,  0.15205475])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x1과 0.3을 곱하여 y(label)에 할당합니다.\n",
    "y = 0.3 * x1\n",
    "\n",
    "# y의 shape를 확인합니다.\n",
    "# 100개가 잘 뽑혔다면 (100, 1) 또는 (100, )이 출력됩니다.\n",
    "print(y.shape)\n",
    "\n",
    "# y의 첫 10개 값을 확인합니다.\n",
    "# 모든 x1의 값에 0.3이 곱해진 것을 확인할 수 있습니다.\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 알고리즘을 학습하는 방법, 즉 가장 성능이 좋은 가중치(Weight)를 구하는 방법으로 크게 3가지, 순서대로 **1) 랜덤 서치**, **2) h-step 서치**, 그리고 **3) gradient descent**을 적용해보며, 이 중에 어떤 것이 가장 좋고 어떤 것을 선택해야 하는지를 배웁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Idea: Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째로 적용해볼 학습 알고리즘은 **랜덤 서치(Random Search)**입니다. 말 그대로 무작위로 **가중치(w1)**을 지정하며 최적의 가중치를 찾는 알고리즘입니다. 무작위로 뽑은 **w1**중에 **x1**과 곱한값(y_predcit)이 __y__에 가장 가깝게 하는, 즉 오차(error)를 가장 작게 하는 w1이 **최적의 w1**가 됩니다. y의 값이 -1에서 +1 사이이므로 **w1**의 범위를 -1에서 +1로 한정하겠습니다. 범위를 좁혀야 문제를 빨리 풀 수 있기 때문입니다. 또한 모든 알고리즘의 반복 횟수(epoch)는 10000으로 통일하고 오차는 [MAE(Mean Absolute Error)](https://medium.com/@ewuramaminka/mean-absolute-error-mae-sample-calculation-6eed6743838a)로 구하겠습니다.\n",
    "\n",
    "랜덤 서치 알고리즘의 순서는 다음과 같습니다.\n",
    "\n",
    "```\n",
    "1. 몇 개의 w1를 랜덤으로 뽑을지 정한다. 즉, 몇 번(epoch) for문을 돌지 정한다.\n",
    "2. 랜덤으로 w1을 구한다.\n",
    "3. w1과 x1을 곱한다.\n",
    "4. 3의 결과(y_predict)와 y와의 오차를 구한다. \n",
    "5. 4에선 구한 오차가 이전에 구했던 오차보다 작으면 최적의 w1을 현재 w1으로 지정한다.\n",
    "6. 2~5번을 epoch만큼 반복한다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =    0, w1 = 0.88899, error = 0.28790\n",
      "epoch =    1, w1 = -0.07180, error = 0.18174\n",
      "epoch =    3, w1 = 0.16379, error = 0.06658\n",
      "epoch =    4, w1 = 0.24017, error = 0.02925\n",
      "epoch =   11, w1 = 0.25480, error = 0.02209\n",
      "epoch =   36, w1 = 0.28054, error = 0.00951\n",
      "epoch =  128, w1 = 0.28602, error = 0.00683\n",
      "epoch =  148, w1 = 0.30717, error = 0.00350\n",
      "epoch =  169, w1 = 0.30209, error = 0.00102\n",
      "epoch =  515, w1 = 0.30043, error = 0.00021\n",
      "epoch = 1743, w1 = 0.30002, error = 0.00001\n",
      "------------------------------------------------------------\n",
      "epoch = 1743, w1 = 0.30002, error = 0.00001\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행할 때마다 결과가 달라지지 않게 하기위하여 seed를 고정합니다.\n",
    "# seed는 임의로 37로 고정하겠습니다.\n",
    "np.random.seed(37)\n",
    "\n",
    "# w1를 찾는 횟수인 num_epoch을 10000으로 지정합니다. 즉, 10000개의 w1중에서 최적의 w1을 찾을 것입니다.\n",
    "num_epoch = 10000\n",
    "\n",
    "# 10000개의 각 w1마다 구해질 error중 가장 작은 에러를 담을 best_error란 변수를 만듭니다.\n",
    "# best_error에 무한대를 할당해 초기화합니다.\n",
    "# numpy의 inf는 무한대를 의미합니다. 무한대는 어떠한 수보다도 크다는 특징을 갖고 있습니다.\n",
    "# 이렇게 해야 첫 번째 error가 어떤 값이 나오든 best_error에 저장될 수 있습니다.\n",
    "best_error = np.inf\n",
    "\n",
    "# 최적의 w1이 몇번 째 구한 것인지 알기위해 best_epoch이란 변수를 만듭니다.\n",
    "# 변수를 만들고 싶지만 아무 값도 할당하지 않고 싶다면 None을 할당하면 됩니다.\n",
    "best_epoch = None\n",
    "\n",
    "# 최적의 w1을 저장할 best_w1이란 변수를 만들고 None을 할당합니다.\n",
    "best_w1 = None\n",
    "\n",
    "# epoch만큼 반복되는 for문을 선언합니다.\n",
    "# for문이 한 번 반복될 때마다 w1을 랜덤으로 한 개 만들고 이전의 error와 비교한 뒤\n",
    "# error가 이전보다 작아졌다면 best_error, best_epoch, best_w1은 현재의 error, epoch, w1으로 저장할 것입니다.\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    # -1 ~ +1 사이에서 랜덤으로 값을 한 개 뽑아 w1에 저장합니다.\n",
    "    w1 = np.random.uniform(low=-1.0, high=1.0)\n",
    "\n",
    "    # x1과 w1의 곱을 y_predict 변수에 저장합니다.\n",
    "    # y_predict는 y를 예측한 값을 뜻합니다.\n",
    "    y_predict = w1 * x1\n",
    "    \n",
    "    # 정답(y)과 예측값(y_predict)의 오차를 구합니다.\n",
    "    # 오차는 MAE(Mean absolute error)를 사용하여 구합니다.\n",
    "    # y_predict에서 y를 뺀 뒤 error에 저장합니다.\n",
    "    error = y_predict - y\n",
    "    # 절댓값을 씌어 y_predict와 y의 절대적인 차이를 구한 뒤 error에 저장합니다.\n",
    "    # numpy의 abs()를 사용합니다.\n",
    "    error = np.abs(error)\n",
    "    # 위에서 구한 절대적인 차이의 평균을 구한 뒤 error에 저장합니다.\n",
    "    # numpy의 mean()을 사용합니다.\n",
    "    # 여기서 구한 값이 최종적으로 error가 됩니다.\n",
    "    error = np.mean(error)\n",
    "    \n",
    "    # 이번 반복에서 구한 error와 best_error를 비교합니다.\n",
    "    # 만약 현재 error가 더 작다면 \n",
    "    # best_error, best_epoch, best_w1를 현재의 error, epoch, w1으로 갱신합니다.\n",
    "    if error < best_error:\n",
    "        \n",
    "        # best_error를 현재의 error로 갱신합니다.\n",
    "        best_error = error\n",
    "        # best_epoch를 현재의 epoch로 갱신합니다.\n",
    "        best_epoch = epoch\n",
    "        # best_w1를 현재의 w1로 갱신합니다.\n",
    "        best_w1 = w1\n",
    "\n",
    "        # 현재의 epoch, w1, error를 print하여 확인합니다.\n",
    "        print(\"epoch = {0:4}, w1 = {1:.5f}, error = {2:.5f}\".format(epoch, w1, error))\n",
    "\n",
    "# best_epoch, best_w1, best_error를 print하여 확인합니다.\n",
    "print(\"----\" * 15)\n",
    "print(\"epoch = {0:4}, w1 = {1:.5f}, error = {2:.5f}\".format(best_epoch, best_w1, best_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Search**의 결과를 보면 error가 0.00001까지 낮아진 것을 확인할 수 있습니다. **Random Search**는 이렇게 좋은 결과를 낼 수 있지만 단순한 랜덤이라는 특징 때문에 epoch을 크게해야 좋은 결과를 낼 수 있습니다. 하지만 epoch을 크게 할 수록 실행속도가 느려진다는 단점이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Idea - h-step Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 번째로 적용해볼 학습 알고리즘은 **h-step Search**입니다. 이 알고리즘은 처음에 랜덤으로 **가중치(w1)**를 지정한다음 __h__만큼 w1를 크거나 작게하고 오차(error)를 구한 뒤 이전의 오차와 비교하는 것을 반복하며 가장 error가 작은 **최적의 w1**를 찾습니다. 즉, **h 간격(step)**만큼 이동하며 **최적의 w1**를 찾는 알고리즘입니다. h를 어떤걸로 세팅하냐에 따라서 이동하는 속도가 달라지는데, h가 너무 크면 속도가 빠르지면 정확하지 않고 h가 너무 작으면 정확하지만 속도가 너무 느립니다. 따라서 어떤 값을 h로 지정할지가 중요한데, 이 h값을 찾는게 어렵습니다. 결론적으로 h값만 잘 정한다면 **random search**보다 빠르게 최적의 w1을 찾을 수 있습니다.\n",
    "\n",
    "`random search`와 마찬가지로 첫 w1의 범위를 -1에서 +1로 한정하겠습니다. 범위를 좁혀야 문제를 빨리 풀 수 있기 때문입니다. 또한 모든 알고리즘의 반복 횟수(epoch)는 10000으로 통일하고 오차는 [MAE(Mean Absolute Error)](https://medium.com/@ewuramaminka/mean-absolute-error-mae-sample-calculation-6eed6743838a)로 구하겠습니다.\n",
    "\n",
    "**h-step Search** 알고리즘의 순서는 다음과 같습니다.\n",
    "\n",
    "```\n",
    "1. 몇 번(epoch) h만큼 step을 이동하며 w1을 구할지, 즉 몇 번 for문을 돌지 정한다.\n",
    "2. 첫 w1을 랜덤으로 지정한다.\n",
    "3. 현재의 w1에 x1을 곱한 뒤(y_predict), y와의 오차를 구한다. 이 오차는 현재 오차가 된다.\n",
    "4. w1에 h를 더한 뒤, y와의 오차를 구한다.\n",
    "5. 4에서 구한 오차가 현재 오차보다 더 작다면 w1을 w1+h 로 갱신한다.\n",
    "6. 반대로 4에서 구한 오차가 더 크다면 w1에 h를 뺀 뒤, y와의 오차를 구한다.\n",
    "7. 6에서 구한 오차가 현재 오차보다 더 작다면 w1을 w1-h 로 갱신한다.\n",
    "8. 3~7번을 epoch만큼 반복한다. 오차가 일정이하로 내려가면 중단한다.\n",
    "```\n",
    "\n",
    "h값에 따라 오차가 일정 수준 이하로 내려가지 못할 수 있습니다. 따라서 의미없이 알고리즘이 실행되는 것을 막기위해 오차가 특정 수준 이하로 내려가면 알고리즘을 중단합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, w1 = -0.42789, error = 0.35580\n",
      "epoch = 1, w1 = -0.41789, error = 0.35091\n",
      "epoch = 2, w1 = -0.40789, error = 0.34602\n",
      "epoch = 3, w1 = -0.39789, error = 0.34113\n",
      "epoch = 4, w1 = -0.38789, error = 0.33624\n",
      "epoch = 5, w1 = -0.37789, error = 0.33136\n",
      "epoch = 6, w1 = -0.36789, error = 0.32647\n",
      "epoch = 7, w1 = -0.35789, error = 0.32158\n",
      "epoch = 8, w1 = -0.34789, error = 0.31669\n",
      "epoch = 9, w1 = -0.33789, error = 0.31180\n",
      "epoch = 10, w1 = -0.32789, error = 0.30692\n",
      "epoch = 11, w1 = -0.31789, error = 0.30203\n",
      "epoch = 12, w1 = -0.30789, error = 0.29714\n",
      "epoch = 13, w1 = -0.29789, error = 0.29225\n",
      "epoch = 14, w1 = -0.28789, error = 0.28736\n",
      "epoch = 15, w1 = -0.27789, error = 0.28248\n",
      "epoch = 16, w1 = -0.26789, error = 0.27759\n",
      "epoch = 17, w1 = -0.25789, error = 0.27270\n",
      "epoch = 18, w1 = -0.24789, error = 0.26781\n",
      "epoch = 19, w1 = -0.23789, error = 0.26292\n",
      "epoch = 20, w1 = -0.22789, error = 0.25804\n",
      "epoch = 21, w1 = -0.21789, error = 0.25315\n",
      "epoch = 22, w1 = -0.20789, error = 0.24826\n",
      "epoch = 23, w1 = -0.19789, error = 0.24337\n",
      "epoch = 24, w1 = -0.18789, error = 0.23848\n",
      "epoch = 25, w1 = -0.17789, error = 0.23360\n",
      "epoch = 26, w1 = -0.16789, error = 0.22871\n",
      "epoch = 27, w1 = -0.15789, error = 0.22382\n",
      "epoch = 28, w1 = -0.14789, error = 0.21893\n",
      "epoch = 29, w1 = -0.13789, error = 0.21404\n",
      "epoch = 30, w1 = -0.12789, error = 0.20915\n",
      "epoch = 31, w1 = -0.11789, error = 0.20427\n",
      "epoch = 32, w1 = -0.10789, error = 0.19938\n",
      "epoch = 33, w1 = -0.09789, error = 0.19449\n",
      "epoch = 34, w1 = -0.08789, error = 0.18960\n",
      "epoch = 35, w1 = -0.07789, error = 0.18471\n",
      "epoch = 36, w1 = -0.06789, error = 0.17983\n",
      "epoch = 37, w1 = -0.05789, error = 0.17494\n",
      "epoch = 38, w1 = -0.04789, error = 0.17005\n",
      "epoch = 39, w1 = -0.03789, error = 0.16516\n",
      "epoch = 40, w1 = -0.02789, error = 0.16027\n",
      "epoch = 41, w1 = -0.01789, error = 0.15539\n",
      "epoch = 42, w1 = -0.00789, error = 0.15050\n",
      "epoch = 43, w1 = 0.00211, error = 0.14561\n",
      "epoch = 44, w1 = 0.01211, error = 0.14072\n",
      "epoch = 45, w1 = 0.02211, error = 0.13583\n",
      "epoch = 46, w1 = 0.03211, error = 0.13095\n",
      "epoch = 47, w1 = 0.04211, error = 0.12606\n",
      "epoch = 48, w1 = 0.05211, error = 0.12117\n",
      "epoch = 49, w1 = 0.06211, error = 0.11628\n",
      "epoch = 50, w1 = 0.07211, error = 0.11139\n",
      "epoch = 51, w1 = 0.08211, error = 0.10651\n",
      "epoch = 52, w1 = 0.09211, error = 0.10162\n",
      "epoch = 53, w1 = 0.10211, error = 0.09673\n",
      "epoch = 54, w1 = 0.11211, error = 0.09184\n",
      "epoch = 55, w1 = 0.12211, error = 0.08695\n",
      "epoch = 56, w1 = 0.13211, error = 0.08207\n",
      "epoch = 57, w1 = 0.14211, error = 0.07718\n",
      "epoch = 58, w1 = 0.15211, error = 0.07229\n",
      "epoch = 59, w1 = 0.16211, error = 0.06740\n",
      "epoch = 60, w1 = 0.17211, error = 0.06251\n",
      "epoch = 61, w1 = 0.18211, error = 0.05763\n",
      "epoch = 62, w1 = 0.19211, error = 0.05274\n",
      "epoch = 63, w1 = 0.20211, error = 0.04785\n",
      "epoch = 64, w1 = 0.21211, error = 0.04296\n",
      "epoch = 65, w1 = 0.22211, error = 0.03807\n",
      "epoch = 66, w1 = 0.23211, error = 0.03319\n",
      "epoch = 67, w1 = 0.24211, error = 0.02830\n",
      "epoch = 68, w1 = 0.25211, error = 0.02341\n",
      "epoch = 69, w1 = 0.26211, error = 0.01852\n",
      "epoch = 70, w1 = 0.27211, error = 0.01363\n",
      "epoch = 71, w1 = 0.28211, error = 0.00875\n",
      "epoch = 72, w1 = 0.29211, error = 0.00386\n",
      "------------------------------------------------------------\n",
      "epoch = 72, w1 = 0.29211, error = 0.00386\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행할 때마다 결과가 달라지지 않게 하기위하여 seed를 고정합니다.\n",
    "# seed는 임의로 37로 고정하겠습니다.\n",
    "np.random.seed(31)\n",
    "\n",
    "# h만큼 이동하는 횟수인 epoch을 10000으로 지정합니다.\n",
    "num_epoch = 10000\n",
    "\n",
    "# -1 ~ +1 사이의 값중 랜덤으로 w1을 지정(초기화)합니다.\n",
    "w1 = np.random.uniform(low=-1.0, high=1.0)\n",
    "\n",
    "# 이동하는 간격(step)인 h를 0.01로 지정합니다.\n",
    "# w1이 -1 ~ +1사이의 값이기 때문에 적당히 작은 값으로 지정하였습니다.\n",
    "h = 0.01\n",
    "\n",
    "# epoch만큼 반복되는 for문을 선언합니다.\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    # 현재의 w1과 x1을 곱하여 y_predict 변수에 저장합니다.\n",
    "    # y_predict는 y를 예측한 값을 뜻합니다.\n",
    "    y_predict = w1 * x1\n",
    "    # 현재 오차를 구하여 current_error에 저장합니다.\n",
    "    # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "    current_error = np.mean(np.abs(y_predict - y))\n",
    "    \n",
    "    # 현재의 epoch, w1, current_error를 print하여 확인합니다.\n",
    "    print(\"epoch = {0}, w1 = {1:.5f}, error = {2:.5f}\".format(epoch, w1, current_error))\n",
    "    \n",
    "    # 만약 현재 오차가 0.005보다 작다면 for문을 마치고 알고리즘을 끝냅니다.\n",
    "    # h값에 따라 오차가 일정 이하로 내려가지 못하는 경우가 생깁니다.\n",
    "    # 이 경우 for문이 의미없이 반복되는 것을 막기 위한 코드입니다.\n",
    "    # 0.005를 0.0001과 같이 더 작은 값으로 한다면 for문이 의미없이 반복되는 것을 확인할 수 있습니다.\n",
    "    if current_error < 0.005:\n",
    "        # break는 for문을 바로 마칩니다.\n",
    "        break\n",
    "\n",
    "    # h만큼 w가 커지는 방향으로 이동합니다. 즉, w1에 h를 더하여 y_predict를 구합니다.\n",
    "    y_predict = (w1 + h) * x1\n",
    "    # 이 때의 오차를 구한 뒤 h_plus_error에 저장합니다.\n",
    "    # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "    h_plus_error = np.mean(np.abs(y_predict - y))\n",
    "    \n",
    "    # 현재 오차인 current_error와 h만큼 이동했을 때의 오차인 h_plus_error를 비교합니다.\n",
    "    # h_plus_error가 더 작다면 w1을 w1+h로 갱신합니다.\n",
    "    if h_plus_error < current_error:\n",
    "        w1 = w1 + h\n",
    "        \n",
    "    # h_plus_error가 더 크다면 이번엔 h만큼 w가 작아지는 방향으로 이동합니다. 즉, w1에 h를 빼고 y_predict를 구합니다.   \n",
    "    else:\n",
    "        # w1에 h를 빼고 y_predict를 구합니다. \n",
    "        y_predict = (w1 - h) * x1\n",
    "        # 이 때의 오차를 구한 뒤 h_minus_error에 저장합니다.\n",
    "        # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "        h_minus_error = np.mean(np.abs(y_predict - y))\n",
    "        \n",
    "        # h_plus_error가 더 작다면 w1을 w1-h로 갱신합니다.\n",
    "        if h_minus_error < current_error:\n",
    "            w1 = w1 - h\n",
    "            \n",
    "# 최종 결과를 확인합니다. \n",
    "# epoch, w1, current_error를 출력합니다.\n",
    "print(\"----\" * 15)   \n",
    "print(\"epoch = {0}, w1 = {1:.5f}, error = {2:.5f}\".format(epoch, w1, current_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비록 **Random Search**에서 달성한 오차인 0.00001보다 큰 오차를 구하였지만 72번의 실행만에 이 오차에 도달한 것을 알 수 있습니다. 즉, **h-step Search**는 **Random Search**보다 효율적으로 학습한다는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Idea - Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세 번째로 적용해볼 학습 알고리즘은 **Gradient Descent**입니다. 이 알고리즘은 PPT로 더 자세히 알아볼 것이므로 지금은 간단하게 알아보겠습니다. **Gradient Descent**는 **h-step Search**와 기본적으로 처음에 랜덤으로 가중치(w1)를 지정한다음 w1을 조금씩 이동하며(값을 바꾸며) error을 줄여나간다는 점에서 동일합니다. 다만 w1을 이동하는 방법에서 차이가 있습니다. **Gradient Descent**는 현재 정답(y)과 예측값(y_predict)와의 차이를 고려하여 w1을 이동한다고 생각하면 됩니다. 가령 x의 값들이 양수(+)라고 가정할 때, y_predict가 y보다 클수록 w1을 작게 하여 y_predict를 감소시키고 y_predict가 y보다 작을수록 w1을 크게 하여 y_predict를 증가시킨다고 생각하면 됩니다. 더 정확하게는 수학적으로 기울기 또는 변화율을 의미하는 Gradient와 Cost Function(비용함수)를 고려하여 w1의 이동방향과 간격을 정하는 알고리즘이지만 이는 PPT에서 다루도록 하고 간단하게 그림만 보고 가겠습니다.\n",
    "\n",
    "<img src = \"https://drive.google.com/uc?export=view&id=1Du4wnbtt9d8v86FZwZf_fJC7dxUGVbyY\" height = \"600\" width = \"500\" />\n",
    "\n",
    "그림으로 **Gradient Descent**를 간단하게 보자면 위와 같이 gradient(기울어진 정도)를 따라서 w를 이동하며 cost(error)를 감소시킵니다.\n",
    "\n",
    "\n",
    "**Gradient Descent**의 장점은 h값을 스스로 조절하기 때문에 **h-step Search**보다 정답에 빠르게 도달할 수 있다는 것입니다. 따라서 딥러닝에에서 학습 알고리즘으로 가장 많이 응용되어 사용되고 있는 알고리즘입니다.\n",
    "\n",
    "\n",
    "\n",
    "**h-step Search**와 마찬가지로 첫 w1의 범위를 -1에서 +1로 한정하겠습니다. 범위를 좁혀야 문제를 빨리 풀 수 있기 때문입니다. 또한 모든 알고리즘의 반복 횟수(epoch)는 10000으로 통일하고 오차는 [MAE(Mean Absolute Error)](https://medium.com/@ewuramaminka/mean-absolute-error-mae-sample-calculation-6eed6743838a)로 구하겠습니다.\n",
    "\n",
    "Gradient Descent 알고리즘의 순서는 다음과 같습니다.\n",
    "\n",
    "```\n",
    "1. 몇 번(epoch) 이동하며 w1을 구할지, 즉 몇 번 for문을 돌지 정한다.\n",
    "2. 첫 w1을 랜덤으로 지정한다.\n",
    "3. 현재의 w1에 x1을 곱한 뒤, y와의 오차를 구한다. 이 오차는 현재 오차가 된다.\n",
    "4. w1을 이동한다. 즉, w1을 w1- np.mean((y_predict - y) * x1))로 갱신한다.\n",
    "5. 3~5번을 epoch만큼 반복한다. 오차가 일정이하로 내려가면 중단한다.\n",
    "```\n",
    "\n",
    "**Gradient Descent** 역시 **h-step Search**와 같이 오차가 일정 수준 이하로 내려가지 못할 수 있습니다. 따라서 의미없이 알고리즘이 실행되는 것을 막기위해 오차가 특정 수준 이하로 내려가면 알고리즘을 중단합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0, w1 = 0.69560, error = 0.28790\n",
      "epoch = 10, w1 = 0.30739, error = 0.00538\n",
      "------------------------------------------------------------\n",
      "epoch = 11, w1 = 0.30739, error = 0.00361\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행할 때마다 결과가 달라지지 않게 하기위하여 seed를 고정합니다.\n",
    "# seed는 임의로 37로 고정하겠습니다.\n",
    "np.random.seed(37)\n",
    "\n",
    "# h만큼 이동하는 횟수인 epoch을 10000으로 지정합니다.\n",
    "num_epoch = 10000\n",
    "\n",
    "# -1 ~ +1 사이의 값중 랜덤으로 w1을 지정(초기화)합니다.\n",
    "w1 = np.random.uniform(low=-1.0, high=1.0)\n",
    "\n",
    "# epoch만큼 반복되는 for문을 선언합니다.\n",
    "for epoch in range(num_epoch):\n",
    " \n",
    "    # 현재의 w1과 x1을 곱하여 y_predict 변수에 저장합니다.\n",
    "    # y_predict는 y를 예측한 값을 뜻합니다.\n",
    "    y_predict = w1 * x1\n",
    "    \n",
    "    # 현재 오차를 구하여 error에 저장합니다.\n",
    "    # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "    error = np.mean(np.abs(y_predict - y))\n",
    "    \n",
    "    \n",
    "    # 만약 현재 오차가 0.005보다 작다면 for문을 마치고 알고리즘을 끝냅니다.\n",
    "    # h값에 따라 오차가 일정 이하로 내려가지 못하는 경우가 생깁니다.\n",
    "    # 이 경우 for문이 의미없이 반복되는 것을 막기 위한 코드입니다.\n",
    "    # 0.005를 0.0001과 같이 더 작은 값으로 한다면 for문이 의미없이 반복되는 것을 확인할 수 있습니다.    \n",
    "    if error < 0.005:\n",
    "        # break는 for문을 바로 마칩니다.\n",
    "        break\n",
    "\n",
    "    # w1을 w1- np.mean((y_predict - y) * x1))로 갱신합니다.\n",
    "    w1 = w1 - np.mean(((y_predict - y) * x1))\n",
    "\n",
    "    # 10회마다 epoch, w1, error를 print하여 확인합니다.\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"epoch = {0:2}, w1 = {1:.5f}, error = {2:.5f}\".format(epoch, w1, error))\n",
    "    \n",
    "    \n",
    "# 최종 결과를 확인합니다. \n",
    "# epoch, w1, current_error를 출력합니다.\n",
    "print(\"----\" * 15)\n",
    "print(\"epoch = {0:2}, w1 = {1:.5f}, error = {2:.5f}\".format(epoch, w1, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**알고리즘은 **h-step Search**보다 작은 오차를 더욱 빠르게 달성한 것을 볼 수 있습니다. 즉, **Gradient Descent**는 **h-step Search**보다 성능이 좋은 알고리즘이란 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2 -  ` y = 0.3 * x1 + 0.5 * x2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 두 개의 **features(x1, x2)**로 이루어진 데이터 셋으로 동일하게 알고리즘을 알아보겠습니다. 두 번째 문제(Case 2)는 두 개의 **features(x1, x2)**로 이루어진 데이터셋(입력값)과 학습할 **가중치(w1, w2)**, 그리고 정답지인 **Label(y)**로 이루어져 있습니다. `y = 0.3 * x1 + 0.5 * x2`이며 `w1 * x1 + w2 * x2`이 __y__에 최대한 가까워 지도록 **w1, w2**을 학습할 것입니다. 즉, 각 각 0.3, 0.5에 최대한 근접한 최적의 w1, w2을 구하는 문제입니다.\n",
    "이 문제를 다시 **1) Random Search, 2) h-step Search, 3) Gradient Descent**로 풀어보겠습니다.\n",
    "\n",
    "x1, x2은 -1 ~ +1 사이의 랜덤한 값(데이터) 100개로 이루어진 데이터셋으로 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.88899321, -0.07180365, -0.61440994,  0.16378975,  0.24016842,\n",
       "        0.36844803, -0.79312489,  0.49095153, -0.43604186,  0.50684917])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy의 사용하여 랜덤한 값을 만들 것입니다.\n",
    "# numpy의 random(np.random)은 랜덤 값을 생성하는 기능들을 포함하고 있습니다.\n",
    "# 코드를 실행할 때마다 랜덤으로 값이 만들어지기 때문에 결과를 고정하기 위해 seed를 고정합니다.\n",
    "# 고정한 seed라 영향을 미치는 범위는 같은 셀(cell)에서 실행되는 random입니다.\n",
    "# seed 숫자는 임의적으로 37로 고정하겠습니다. \n",
    "np.random.seed(37)\n",
    "\n",
    "# np.random.uniform()은 최솟값, 최댓값, 개수를 입력받습니다.\n",
    "# 최솟값과 최댓값 사이에서 지정한 개수만큼 랜덤하게 실수(Real Number)를 추출합니다.\n",
    "# x1은 Case 1에서 만들었으므로 x2만 추가적으로 만듭니다.\n",
    "x2 = np.random.uniform(low=-1.0, high=1.0, size=100)\n",
    "\n",
    "# x2의 shape를 확인합니다.\n",
    "# 100개가 잘 뽑혔다면 (100, 1) 또는 (100, )이 출력됩니다.\n",
    "print(x2.shape)\n",
    "\n",
    "# x1의 첫 10개 값을 확인합니다.\n",
    "x2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.71119456, -0.05744292, -0.49152795,  0.1310318 ,  0.19213474,\n",
       "        0.29475843, -0.63449991,  0.39276123, -0.34883349,  0.40547934])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x1과 w1을 곱한 값과 x2와 w2를 곱한 값을 더한 후 y(label)에 할당합니다.\n",
    "y = 0.3 * x1 + 0.5 * x2\n",
    "\n",
    "# y의 shape를 확인합니다.\n",
    "# 100개가 잘 뽑혔다면 (100, 1) 또는 (100, )이 출력됩니다.\n",
    "print(y.shape)\n",
    "\n",
    "# y의 첫 10개 값을 확인합니다.\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First idea: Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째로 적용해볼 학습 알고리즘은 **랜덤 서치(Random Search)**입니다. 말 그대로 무작위로 **가중치(w1, w2)**을 지정하는 방법입니다. 무작위로 뽑은 **w1, w2**중에 `w1 * x1 + w2 * x2` 값이 __y__에 가장 가깝게 하는, 즉 오차를 가장 작게 하는 w1, w2이 **최적의 w1, w2**가 됩니다. y의 값이 -1에서 +1 사이이므로 **w1, w2**의 범위를 -1에서 +1로 한정하겠습니다. 범위를 좁혀야 문제를 빨리 풀 수 있기 때문입니다. 또한 모든 알고리즘의 반복 횟수(epoch)는 10000으로 통일하고 오차는 [MAE(Mean Absolute Error)](https://medium.com/@ewuramaminka/mean-absolute-error-mae-sample-calculation-6eed6743838a)로 구하겠습니다.\n",
    "\n",
    "랜덤 서치 알고리즘의 순서는 다음과 같습니다.\n",
    "\n",
    "```\n",
    "1. 몇 개의 w1, w2를 랜덤으로 뽑을지 정한다. 즉, 몇 번(epoch) for문을 돌지 정한다.\n",
    "2. 랜덤으로 w1, w2을 구한다.\n",
    "3. w1 * x1 + w2 * x2를 구한다.\n",
    "4. 3의 결과(y_predict)와 y와의 오차를 구한다. \n",
    "5. 4에선 구한 오차가 이전에 구했던 오차보다 작으면 최적의 w1, w2을 현재 w1으로 지정한다.\n",
    "6. 2~5번을 epoch만큼 반복한다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =    0, w1 = 0.88899, w2 = -0.07180, error = 0.00840\n",
      "epoch =    6, w1 = -0.11302, w2 = 0.92693, error = 0.00680\n",
      "epoch =  230, w1 = 0.30341, w2 = 0.48725, error = 0.00457\n",
      "epoch =  421, w1 = 0.99932, w2 = -0.20003, error = 0.00035\n",
      "epoch = 2755, w1 = 0.13833, w2 = 0.66159, error = 0.00004\n",
      "------------------------------------------------------------\n",
      "epoch = 2755, w1 = 0.13833, w2 = 0.66159, error = 0.00004\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행할 때마다 결과가 달라지지 않게 하기위하여 seed를 고정합니다.\n",
    "# seed는 임의로 37로 고정하겠습니다.\n",
    "np.random.seed(37)\n",
    "\n",
    "# w1를 찾는 횟수인 epoch을 10000으로 지정합니다. 즉, 10000개의 w1중에서 최적의 w1을 찾을 것입니다.\n",
    "num_epoch = 10000\n",
    "\n",
    "# 10000개의 각 w1마다 구해질 error중 가장 작은 에러를 담을 best_error란 변수를 만듭니다.\n",
    "# best_error에 무한대를 할당해 초기화합니다.\n",
    "# numpy의 inf는 무한대를 의미합니다. 무한대는 어떠한 수보다도 크다는 특징을 갖고 있습니다.\n",
    "# 이렇게 해야 첫 번째 error가 어떤 값이 나오든 best_error에 저장될 수 있습니다.\n",
    "best_error = np.inf\n",
    "\n",
    "# 최적의 w1이 몇번 째 구한 것인지 알기위해 best_epoch이란 변수를 만듭니다.\n",
    "# 변수를 만들고 싶지만 아무 값도 할당하지 않고 싶다면 None을 할당하면 됩니다.\n",
    "best_epoch = None\n",
    "\n",
    "# 최적의 w1을 저장할 best_w1이란 변수를 만들고 None을 할당합니다.\n",
    "best_w1 = None\n",
    "# 최적의 w2을 저장할 best_w2이란 변수를 만들고 None을 할당합니다.\n",
    "best_w2 = None\n",
    "\n",
    "# epoch만큼 반복되는 for문을 선언합니다.\n",
    "# for문이 한 번 반복될 때마다 w1을 랜덤으로 한 개 만들고 이전의 error와 비교한 뒤\n",
    "# 에러가 이전보다 작아졌다면 best_error, best_epoch, best_w1은 현재의 error, epoch, w1으로 저장할 것입니다.\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    # -1 ~ +1 사이에서 랜덤으로 값을 한 개 뽑아 w1에 저장합니다.\n",
    "    w1 = np.random.uniform(low=-1.0, high=1.0)\n",
    "    # -1 ~ +1 사이에서 랜덤으로 값을 한 개 뽑아 w2에 저장합니다.\n",
    "    w2 = np.random.uniform(low=-1.0, high=1.0)\n",
    "\n",
    "\n",
    "    # x1과 w1의 곱과 x2와 w2의 곱을 더한 것을 y_predict 변수에 저장합니다.\n",
    "    # y_predict는 y를 예측한 값을 뜻합니다.\n",
    "    y_predict = w1 * x1 + w2 * x2 \n",
    "    \n",
    "    # 정답(y)과 예측값(y_predict)의 오차를 구합니다.\n",
    "    # 오차는 MAE(Mean absolute error)를 사용하여 구합니다.\n",
    "    error = np.mean(np.abs(y_predict - y))\n",
    "    \n",
    "    # 이번 반복에서 구한 error와 best_error를 비교합니다.\n",
    "    # 만약 현재 error가 더 작다면 \n",
    "    # best_error, best_epoch, best_w1를 현재의 error, epoch, w1으로 갱신합니다.\n",
    "    if error < best_error:\n",
    "        \n",
    "        # best_error를 현재의 error로 갱신합니다.\n",
    "        best_error = error\n",
    "        # best_epoch를 현재의 epoch로 갱신합니다.\n",
    "        best_epoch = epoch\n",
    "        # best_w1를 현재의 w1로 갱신합니다.\n",
    "        best_w1 = w1\n",
    "        # best_w2를 현재의 w2로 갱신합니다.\n",
    "        best_w2 = w2\n",
    "\n",
    "        # 현재의 epoch, w1, w2, error를 print하여 확인합니다.\n",
    "        print(\"epoch = {0:4}, w1 = {1:.5f}, w2 = {2:.5f}, error = {3:.5f}\".format(epoch, w1, w2, error))\n",
    "\n",
    "# best_epoch, best_w1, best_w2, best_error를 print하여 확인합니다.\n",
    "print(\"----\" * 15)\n",
    "print(\"epoch = {0:4}, w1 = {1:.5f}, w2 = {2:.5f}, error = {3:.5f}\".format(best_epoch, best_w1, best_w2, best_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Search**의 결과를 보면 error가 0.00004까지 낮아진 것을 확인할 수 있습니다. **Random Search**는 이렇게 좋은 결과를 낼 수 있지만 단순한 랜덤이라는 특징 때문에 epoch을 크게해야 좋은 결과를 낼 수 있습니다. 하지만 epoch을 크게 할 수록 실행속도가 느려진다는 단점이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Idea - h-step Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 번째로 적용해볼 학습 알고리즘은 **h-step Search**입니다. 이 알고리즘은 처음에 랜덤으로 **가중치(w1, w2)**를 지정한다음 h만큼씩 w1, w2를 크거나 작게하고 error를 구한 뒤 이전의 error와 비교하는 것을 반복하며 가장 error가 작은 최적의 w1, w2를 찾습니다. 즉, h 간격(step)만큼 이동하며 최적의 w1, w2를 찾는 알고리즘입니다. 이 경우 h를 어떤걸로 세팅하냐에 따라서 이동하는 속도가 달라집니다. 즉, h가 너무 크면 속도가 빠르지면 정확하지 않고, h가 너무 작으면 정확하지만 속도가 너무 느립니다. 따라서 어떤 값을 h로 지정할지가 중요한데, 이 h값을 찾는게 어렵습니다. 결론적으로 h값만 잘 정한다면 **random search**보다 빠르게 최적의 w1을 찾을 수 있습니다.\n",
    "\n",
    "`random search`와 마찬가지로 첫 w1, w2의 범위를 -1에서 +1로 한정하겠습니다. 범위를 좁혀야 문제를 빨리 풀 수 있기 때문입니다. 또한 모든 알고리즘의 반복 횟수(epoch)는 10000으로 통일하고 오차는 [MAE(Mean Absolute Error)](https://medium.com/@ewuramaminka/mean-absolute-error-mae-sample-calculation-6eed6743838a)로 구하겠습니다.\n",
    "\n",
    "**h-step Search** 알고리즘의 순서는 다음과 같습니다.\n",
    "\n",
    "```\n",
    "1. 몇 번(epoch) h만큼 step을 이동하며 w1, w2을 구할지, 즉 몇 번 for문을 돌지 정한다.\n",
    "2. 첫 w, w21을 랜덤으로 지정한다.\n",
    "3. 현재의 w1에 x1을 곱한 것과 w2와 x2를 곱한 것을 합(y_predict)한 후, y와의 오차를 구한다. 이 오차는 현재 오차가 된다.\n",
    "4. w1에 h를 더한 뒤, y와의 오차를 구한다.\n",
    "5. 4에서 구한 오차가 현재 오차보다 더 작다면 w1을 w1+h로 갱신한다.\n",
    "6. 반대로 4에서 구한 오차가 더 크다면 w1에 h를 뺀 뒤, y와의 오차를 구한다.\n",
    "7. 6에서 구한 오차가 현재 오차보다 더 작다면 w1을 w1-h로 갱신한다.\n",
    "8. 4~7번을 w2에 대해서 진행한다.\n",
    "9. 3~8번을 epoch만큼 반복한다. 오차가 일정이하로 내려가면 중단한다.\n",
    "```\n",
    "\n",
    "h값에 따라 오차가 일정 수준 이하로 내려가지 못할 수 있습니다. 따라서 의미없이 알고리즘이 실행되는 것을 막기위해 오차가 특정 수준 이하로 내려가면 알고리즘을 중단합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, w1 = -0.42789, w2 = 0.91621, error = 0.15235\n",
      "epoch = 1, w1 = -0.41789, w2 = 0.92621, error = 0.14257\n",
      "epoch = 2, w1 = -0.40789, w2 = 0.93621, error = 0.13280\n",
      "epoch = 3, w1 = -0.39789, w2 = 0.94621, error = 0.12302\n",
      "epoch = 4, w1 = -0.38789, w2 = 0.95621, error = 0.11325\n",
      "epoch = 5, w1 = -0.37789, w2 = 0.96621, error = 0.10347\n",
      "epoch = 6, w1 = -0.36789, w2 = 0.97621, error = 0.09369\n",
      "epoch = 7, w1 = -0.35789, w2 = 0.98621, error = 0.08392\n",
      "epoch = 8, w1 = -0.34789, w2 = 0.99621, error = 0.07414\n",
      "epoch = 9, w1 = -0.33789, w2 = 1.00621, error = 0.06437\n",
      "epoch = 10, w1 = -0.32789, w2 = 1.01621, error = 0.05459\n",
      "epoch = 11, w1 = -0.31789, w2 = 1.02621, error = 0.04481\n",
      "epoch = 12, w1 = -0.30789, w2 = 1.03621, error = 0.03504\n",
      "epoch = 13, w1 = -0.29789, w2 = 1.04621, error = 0.02526\n",
      "epoch = 14, w1 = -0.28789, w2 = 1.05621, error = 0.01549\n",
      "epoch = 15, w1 = -0.27789, w2 = 1.06621, error = 0.00571\n",
      "epoch = 16, w1 = -0.26789, w2 = 1.07621, error = 0.00407\n",
      "------------------------------------------------------------\n",
      "epoch = 16, w1 = -0.26789, w2 = 1.07621, error = 0.00407\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행할 때마다 결과가 달라지지 않게 하기위하여 seed를 고정합니다.\n",
    "# seed는 임의로 37로 고정하겠습니다.\n",
    "np.random.seed(31)\n",
    "\n",
    "# h만큼 이동하는 횟수인 epoch을 10000으로 지정합니다.\n",
    "num_epoch = 10000\n",
    "\n",
    "# -1 ~ +1 사이의 값중 랜덤으로 w1을 지정(초기화)합니다.\n",
    "w1 = np.random.uniform(low=-1.0, high=1.0)\n",
    "# -1 ~ +1 사이의 값중 랜덤으로 w2을 지정(초기화)합니다.\n",
    "w2 = np.random.uniform(low=-1.0, high=1.0)\n",
    "\n",
    "# 이동하는 간격(step)인 h를 0.01로 지정합니다.\n",
    "# w1, w2가 -1 ~ +1사이의 값이기 때문에 적당히 작은 값으로 지정하였습니다.\n",
    "h = 0.01\n",
    "\n",
    "# epoch만큼 반복되는 for문을 선언합니다.\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    # x1과 w1의 곱과 x2와 w2의 곱을 더한 것을 y_predict 변수에 저장합니다.\n",
    "    # y_predict는 y를 예측한 값을 뜻합니다.\n",
    "    y_predict = w1 * x1 + w2 * x2\n",
    "    # 현재 오차를 구하여 current_error에 저장합니다.\n",
    "    # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "    current_error = np.mean(np.abs(y_predict - y))\n",
    "    \n",
    "    # 현재의 epoch, w1, w2, current_error를 print하여 확인합니다.\n",
    "    print(\"epoch = {0}, w1 = {1:.5f}, w2 = {2:.5f}, error = {3:.5f}\".format(epoch, w1, w2, current_error))\n",
    "    \n",
    "    # 만약 현재 오차가 0.005보다 작다면 for문을 마치고 알고리즘을 끝냅니다.\n",
    "    # h값에 따라 오차가 일정 이하로 내려가지 못하는 경우가 생깁니다.\n",
    "    # 이 경우 for문이 의미없이 반복되는 것을 막기 위한 코드입니다.\n",
    "    # 0.005를 0.0001과 같이 더 작은 값으로 한다면 for문이 의미없이 반복되는 것을 확인할 수 있습니다.\n",
    "    if current_error < 0.005:\n",
    "        # break는 for문을 바로 마칩니다.\n",
    "        break\n",
    "\n",
    "    # h만큼 w1가 커지는 방향으로 이동합니다. 즉, w1에 h를 더하여 y_predict를 구합니다.\n",
    "    y_predict = (w1 + h) * x1 + w2 * x2\n",
    "    # 이 때의 오차를 구한 뒤 h_plus_error에 저장합니다.\n",
    "    # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "    h_plus_error = np.mean(np.abs(y_predict - y))\n",
    "    \n",
    "    # 현재 오차인 current_error와 h만큼 이동했을 때의 오차인 h_plus_error를 비교합니다.\n",
    "    # h_plus_error가 더 작다면 w1을 w1+h로 갱신합니다.\n",
    "    if h_plus_error < current_error:\n",
    "        w1 = w1 + h\n",
    "        \n",
    "    # h_plus_error가 더 크다면 이번엔 h만큼 w가 작아지는 방향으로 이동합니다. 즉, w1에 h를 빼고 y_predict를 구합니다.   \n",
    "    else:\n",
    "        # w1에 h를 빼고 y_predict를 구합니다. \n",
    "        y_predict = (w1 - h) * x1 + w2 * x2\n",
    "        # 이 때의 오차를 구한 뒤 h_minus_error에 저장합니다.\n",
    "        # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "        h_minus_error = np.mean(np.abs(y_predict - y))\n",
    "        \n",
    "        # h_plus_error가 더 작다면 w1을 w1-h로 갱신합니다.\n",
    "        if h_minus_error < current_error:\n",
    "            w1 = w1 - h\n",
    "            \n",
    "    # h만큼 w2가 커지는 방향으로 이동합니다. 즉, w2에 h를 더하여 y_predict를 구합니다.\n",
    "    y_predict = w1 * x1 + (w2 + h) * x2\n",
    "    # 이 때의 오차를 구한 뒤 h_plus_error에 저장합니다.\n",
    "    # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "    h_plus_error = np.mean(np.abs(y_predict - y))\n",
    "    \n",
    "    # 현재 오차인 current_error와 h만큼 이동했을 때의 오차인 h_plus_error를 비교합니다.\n",
    "    # h_plus_error가 더 작다면 w2을 w2+h로 갱신합니다.\n",
    "    if h_plus_error < current_error:\n",
    "        w2 = w2 + h\n",
    "        \n",
    "    # h_plus_error가 더 크다면 이번엔 h만큼 w가 작아지는 방향으로 이동합니다. 즉, w2에 h를 빼고 y_predict를 구합니다.   \n",
    "    else:\n",
    "        # w2에 h를 빼고 y_predict를 구합니다. \n",
    "        y_predict = w1 * x1 + (w2 - h) * x2\n",
    "        # 이 때의 오차를 구한 뒤 h_minus_error에 저장합니다.\n",
    "        # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "        h_minus_error = np.mean(np.abs(y_predict - y))\n",
    "        \n",
    "        # h_plus_error가 더 작다면 w2을 w2-h로 갱신합니다.\n",
    "        if h_minus_error < current_error:\n",
    "            w2 = w2 - h\n",
    "            \n",
    "# 최종 결과를 확인합니다. \n",
    "# epoch, w1, w2 current_error를 출력합니다.\n",
    "print(\"----\" * 15)   \n",
    "print(\"epoch = {0}, w1 = {1:.5f}, w2 = {2:.5f}, error = {3:.5f}\".format(epoch, w1, w2, current_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비록 **Random Search**에서 달성한 오차인 0.00004보다 큰 오차를 구하였지만 16번의 실행만에 이 오차에 도달한 것을 알 수 있습니다. 즉, **h-step Search**는 **Random Search**보다 효율적으로 학습한다는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Idea - Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세 번째로 적용해볼 학습 알고리즘은 **Gradient Descent**입니다. 이 알고리즘은 PPT로 더 자세히 알아볼 것이므로 지금은 간단하게 알아보겠습니다. **Gradient Descent**는 **h-step Search**와 기본적으로 처음에 랜덤으로 가중치(w1, w2)를 지정한다음 w1, w2을 조금씩 이동하며(값을 바꾸며) error을 줄여나간다는 점에서 동일합니다. 다만 w1, w2을 이동하는 방법에서 차이가 있습니다. **Gradient Descent**는 현재 정답(y)과 예측값(y_predict)와의 차이를 고려하여 w1, w2을 이동한다고 생각하면 됩니다. 가령 feature(x1, x2)의 값들이 양수(+)라고 가정할 때, y_predict가 y보다 클수록 w1, w2을 작게 하여 y_predict를 감소시키고 y_predict가 y보다 작을수록 w1, w2을 크게 하여 y_predict를 증가시킨다고 생각하면 됩니다. 더 정확하게는 수학적으로 기울기 또는 변화율을 의미하는 Gradient와 Cost Function(비용함수)를 고려하여 w1의 이동방향과 간격을 정하는 알고리짐이지만 이는 PPT에서 다루도록 하겠습니다.\n",
    "\n",
    "**Gradient Descent**의 장점은 h값을 스스로 조절하기 때문에 **h-step Search**보다 정답에 빠르게 도달할 수 있다는 것입니다. 따라서 딥러닝에에서 학습 알고리즘으로 가장 많이 응용되어 사용되고 있는 알고리즘입니다.\n",
    "\n",
    "\n",
    "<img src = \"https://drive.google.com/uc?export=view&id=1Du4wnbtt9d8v86FZwZf_fJC7dxUGVbyY\" height = \"600\" width = \"500\" />\n",
    "\n",
    "그림으로 **Gradient Descent**를 간단하게 보자면 위와 같이 gradient(기울어진 정도)를 따라서 w를 이동하며 cost(error)를 감소시킵니다.\n",
    "\n",
    "\n",
    "\n",
    "**h-step Search**와 마찬가지로 첫 w1, w2의 범위를 -1에서 +1로 한정하겠습니다. 범위를 좁혀야 문제를 빨리 풀 수 있기 때문입니다. 또한 모든 알고리즘의 반복 횟수(epoch)는 10000으로 통일하고 오차는 [MAE(Mean Absolute Error)](https://medium.com/@ewuramaminka/mean-absolute-error-mae-sample-calculation-6eed6743838a)로 구하겠습니다.\n",
    "\n",
    "Gradient Descent 알고리즘의 순서는 다음과 같습니다.\n",
    "\n",
    "```\n",
    "1. 몇 번(epoch) 이동하며 w1, w2을 구할지, 즉 몇 번 for문을 돌지 정한다.\n",
    "2. 첫 w1, w2을 랜덤으로 지정한다.\n",
    "3. 현재의 `w1 * x1 + w2 * x2`(y_predict)와, y와의 오차를 구한다. 이 오차는 현재 오차가 된다.\n",
    "4. w1, w2을 이동한다. 즉, w1을 w1- np.mean((y_predict - y) * x1))로, w2을 w2- np.mean((y_predict - y) * x2))로 갱신한다.\n",
    "5. 3~5번을 epoch만큼 반복한다. 오차가 일정이하로 내려가면 중단한다.\n",
    "```\n",
    "\n",
    "**Gradient Descent** 역시 **h-step Search**와 같이 오차가 일정 수준 이하로 내려가지 못할 수 있습니다. 따라서 의미없이 알고리즘이 실행되는 것을 막기위해 오차가 특정 수준 이하로 내려가면 알고리즘을 중단합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0, w1 = 0.88335, w2 = -0.07745, error = 0.00840\n",
      "------------------------------------------------------------\n",
      "epoch =  1, w1 = 0.88335, w2 = -0.07745, error = 0.00288\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행할 때마다 결과가 달라지지 않게 하기위하여 seed를 고정합니다.\n",
    "# seed는 임의로 37로 고정하겠습니다.\n",
    "np.random.seed(37)\n",
    "\n",
    "# h만큼 이동하는 횟수인 epoch을 10000으로 지정합니다.\n",
    "num_epoch = 10000\n",
    "\n",
    "# -1 ~ +1 사이의 값중 랜덤으로 w1을 지정(초기화)합니다.\n",
    "w1 = np.random.uniform(low=-1.0, high=1.0)\n",
    "# -1 ~ +1 사이의 값중 랜덤으로 w2을 지정(초기화)합니다.\n",
    "w2 = np.random.uniform(low=-1.0, high=1.0)\n",
    "\n",
    "# epoch만큼 반복되는 for문을 선언합니다.\n",
    "for epoch in range(num_epoch):\n",
    " \n",
    "    # x1과 w1의 곱과 x2와 w2의 곱을 더한 것을 y_predict 변수에 저장합니다.\n",
    "    # y_predict는 y를 예측한 값을 뜻합니다.\n",
    "    y_predict = w1 * x1 + w2 * x2\n",
    "    \n",
    "    # 현재 오차를 구하여 error에 저장합니다.\n",
    "    # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "    error = np.mean(np.abs(y_predict - y))\n",
    "    \n",
    "    \n",
    "    # 만약 현재 오차가 0.005보다 작다면 for문을 마치고 알고리즘을 끝냅니다.\n",
    "    # h값에 따라 오차가 일정 이하로 내려가지 못하는 경우가 생깁니다.\n",
    "    # 이 경우 for문이 의미없이 반복되는 것을 막기 위한 코드입니다.\n",
    "    # 0.005를 0.0001과 같이 더 작은 값으로 한다면 for문이 의미없이 반복되는 것을 확인할 수 있습니다.    \n",
    "    if error < 0.005:\n",
    "        # break는 for문을 바로 마칩니다.\n",
    "        break\n",
    "\n",
    "    # w1을 w1- np.mean((y_predict - y) * x1))로 갱신합니다.\n",
    "    w1 = w1 - np.mean(((y_predict - y) * x1))\n",
    "    # w2을 w2- np.mean((y_predict - y) * x2))로 갱신합니다.\n",
    "    w2 = w2 - np.mean(((y_predict - y) * x2))\n",
    "    \n",
    "    # 10회마다 epoch, w1, w2, error를 출력하여 확인합니다.\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"epoch = {0:2}, w1 = {1:.5f}, w2 = {2:.5f}, error = {3:.5f}\".format(epoch, w1, w2, error))\n",
    "    \n",
    "    \n",
    "# 최종 결과를 확인합니다. \n",
    "# epoch, w1, w2, current_error를 출력합니다.\n",
    "print(\"----\" * 15)\n",
    "print(\"epoch = {0:2}, w1 = {1:.5f}, w2 = {2:.5f}, error = {3:.5f}\".format(epoch, w1, w2, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**알고리즘은 **h-step Search**보다 작은 오차를 더욱 빠르게 달성한 것을 볼 수 있습니다. 즉, **Gradient Descent**는 **h-step Search**보다 성능이 좋은 알고리즘이란 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3 - `y = 0.3 * x1 + 0.5 * x2 + 0.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 **Case 2**의 문제에 **bias(편향)**개념을 추가해보겠습니다. 우선 간단하게 bias(b)는 **y값을 전반적으로 편향되게 만든다, 또는 크거나 작게 한다**라고 생각하면 됩니다. `y = 0.3 * x1 + 0.5 * x2 + 0.1`이며, 결론적으로 `w1 * x1 + w2 * x2 + b` 값이 __y__에 최대한 가까워 지도록 **w1, w2, b**을 학습할 것입니다. 즉, 각 각 0.3, 0.5, 0.1에 최대한 근접한 최적의 w1, w2, b을 구하는 문제입니다. 이 문제를 다시 **1) Random Search, 2) h-step Search, 3) Gradient Descent**로 풀어보겠습니다.\n",
    "\n",
    "x1, x2, b1은 -1 ~ +1 사이의 랜덤한 값(데이터) 100개로 이루어진 데이터셋으로 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.81119456,  0.04255708, -0.39152795,  0.2310318 ,  0.29213474,\n",
       "        0.39475843, -0.53449991,  0.49276123, -0.24883349,  0.50547934])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x1과 w1을 곱한 값과 x2와 w2를 곱한 값 그리고 b(bias)를 더한 후 y(label)에 할당합니다.\n",
    "y = 0.3 * x1 + 0.5 * x2 + 0.1\n",
    "\n",
    "# y의 shape를 확인합니다.\n",
    "# 100개가 잘 뽑혔다면 (100, 1) 또는 (100, )이 출력됩니다.\n",
    "print(y.shape)\n",
    "\n",
    "# y의 첫 10개 값을 확인합니다.\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First idea: Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째로 적용해볼 학습 알고리즘은 **랜덤 서치(Random Search)**입니다. 말 그대로 무작위로 **가중치(w1, w2)**와 **bias(b)**를 지정하는 방법입니다. 무작위로 w1, w2, b1을 뽑은 뒤 학습하며 `w1 * x1 + w2 * x2 + b` 값이 y에 가장 가깝게 하는, 즉 오차를 가장 작게 하는 최적의 **w1, w2, b**를 찾습니다. __y__의 값이 -1에서 +1 사이이므로 w1, w2, b의 범위를 -1에서 +1로 한정하겠습니다. 범위를 좁혀야 문제를 빨리 풀 수 있기 때문입니다. 또한 모든 알고리즘의 반복 횟수(epoch)는 10000으로 통일하고 오차는 [MAE(Mean Absolute Error)](https://medium.com/@ewuramaminka/mean-absolute-error-mae-sample-calculation-6eed6743838a)로 구하겠습니다.\n",
    "\n",
    "랜덤 서치 알고리즘의 순서는 다음과 같습니다.\n",
    "\n",
    "```\n",
    "1. 몇 개의 w1, w2를 랜덤으로 뽑을지 정한다. 즉, 몇 번(epoch) for문을 돌지 정한다.\n",
    "2. 랜덤으로 w1, w2을 구한다.\n",
    "3. w1 * x1 + w2 * x2 + b(y_predict)를 구한다.\n",
    "4. 3의 결과(y_predict)와 y와의 오차를 구한다. \n",
    "5. 4에선 구한 오차가 이전에 구했던 오차보다 작으면 최적의 w1, w2, b을 현재의 w1, w2, b으로 지정한다.\n",
    "6. 2~5번을 epoch만큼 반복한다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =    0, w1 = 0.88899, w2 = -0.07180, b = -0.61441, error = 0.71317\n",
      "epoch =    1, w1 = 0.16379, w2 = 0.24017, b = 0.36845, error = 0.26774\n",
      "epoch =    3, w1 = 0.50685, w2 = 0.58541, b = 0.25480, error = 0.20317\n",
      "epoch =   41, w1 = 0.84789, w2 = -0.25580, b = 0.15352, error = 0.10347\n",
      "epoch =   72, w1 = 0.12433, w2 = 0.87523, b = 0.03484, error = 0.10280\n",
      "epoch =  203, w1 = -0.14950, w2 = 0.76058, b = 0.12300, error = 0.09133\n",
      "epoch =  281, w1 = -0.20003, w2 = 0.94236, b = 0.08999, error = 0.02995\n",
      "epoch = 1841, w1 = 0.04118, w2 = 0.80811, b = 0.08408, error = 0.02535\n",
      "epoch = 2083, w1 = 0.33570, w2 = 0.42391, b = 0.10108, error = 0.01968\n",
      "epoch = 7602, w1 = 0.67160, w2 = 0.15323, b = 0.08693, error = 0.01467\n",
      "------------------------------------------------------------\n",
      "epoch = 7602, w1 = 0.67160, w2 = 0.15323, b = 0.08693, error = 0.01467\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행할 때마다 결과가 달라지지 않게 하기위하여 seed를 고정합니다.\n",
    "# seed는 임의로 37로 고정하겠습니다.\n",
    "np.random.seed(37)\n",
    "\n",
    "# w1를 찾는 횟수인 epoch을 10000으로 지정합니다. 즉, 10000개의 w1중에서 최적의 w1을 찾을 것입니다.\n",
    "num_epoch = 10000\n",
    "\n",
    "# 10000개의 각 w1마다 구해질 error중 가장 작은 에러를 담을 best_error란 변수를 만듭니다.\n",
    "# best_error에 무한대를 할당해 초기화합니다.\n",
    "# numpy의 inf는 무한대를 의미합니다. 무한대는 어떠한 수보다도 크다는 특징을 갖고 있습니다.\n",
    "# 이렇게 해야 첫 번째 error가 어떤 값이 나오든 best_error에 저장될 수 있습니다.\n",
    "best_error = np.inf\n",
    "\n",
    "# 최적의 w1이 몇번 째 구한 것인지 알기위해 best_epoch이란 변수를 만듭니다.\n",
    "# 변수를 만들고 싶지만 아무 값도 할당하지 않고 싶다면 None을 할당하면 됩니다.\n",
    "best_epoch = None\n",
    "\n",
    "# 최적의 w1을 저장할 best_w1이란 변수를 만들고 None을 할당합니다.\n",
    "best_w1 = None\n",
    "# 최적의 w2을 저장할 best_w2이란 변수를 만들고 None을 할당합니다.\n",
    "best_w2 = None\n",
    "# 최적의 b을 저장할 best_b란 변수를 만들고 None을 할당합니다.\n",
    "best_b = None\n",
    "\n",
    "# epoch만큼 반복되는 for문을 선언합니다.\n",
    "# for문이 한 번 반복될 때마다 w1을 랜덤으로 한 개 만들고 이전의 error와 비교한 뒤\n",
    "# 에러가 이전보다 작아졌다면 best_error, best_epoch, best_w1은 현재의 error, epoch, w1으로 저장할 것입니다.\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    # -1 ~ +1 사이에서 랜덤으로 값을 한 개 뽑아 w1에 저장합니다.\n",
    "    w1 = np.random.uniform(low=-1.0, high=1.0)\n",
    "    # -1 ~ +1 사이에서 랜덤으로 값을 한 개 뽑아 w2에 저장합니다.\n",
    "    w2 = np.random.uniform(low=-1.0, high=1.0)\n",
    "    # -1 ~ +1 사이에서 랜덤으로 값을 한 개 뽑아 w2에 저장합니다.\n",
    "    b = np.random.uniform(low=-1.0, high=1.0)\n",
    "    \n",
    "\n",
    "\n",
    "    # x1과 w1의 곱, x2와 w2의 곱과 b를 더한 것을 y_predict 변수에 저장합니다.\n",
    "    # y_predict는 y를 예측한 값을 뜻합니다.\n",
    "    y_predict = w1 * x1 + w2 * x2 + b\n",
    "    \n",
    "    # 정답(y)과 예측값(y_predict)의 오차를 구합니다.\n",
    "    # 오차는 MAE(Mean absolute error)를 사용하여 구합니다.\n",
    "    # y_predict에서 y를 뺀 뒤 error에 저장합니다.\n",
    "    error = y_predict - y\n",
    "    # 절댓값을 씌어 y_predict와 y의 절대적인 차이를 구한 뒤 error에 저장합니다.\n",
    "    # numpy의 abs()를 사용합니다.\n",
    "    error = np.abs(error)\n",
    "    # 위에서 구한 절대적인 차이의 평균을 구한 뒤 error에 저장합니다.\n",
    "    # numpy의 mean()을 사용합니다.\n",
    "    # 여기서 구한 값이 최종적으로 error가 됩니다.\n",
    "    error = np.mean(error)\n",
    "    \n",
    "    # 이번 반복에서 구한 error와 best_error를 비교합니다.\n",
    "    # 만약 현재 error가 더 작다면 \n",
    "    # best_error, best_epoch, best_w1를 현재의 error, epoch, w1으로 갱신합니다.\n",
    "    if error < best_error:\n",
    "        \n",
    "        # best_error를 현재의 error로 갱신합니다.\n",
    "        best_error = error\n",
    "        # best_epoch를 현재의 epoch로 갱신합니다.\n",
    "        best_epoch = epoch\n",
    "        # best_w1를 현재의 w1로 갱신합니다.\n",
    "        best_w1 = w1\n",
    "        # best_w2를 현재의 w2로 갱신합니다.\n",
    "        best_w2 = w2\n",
    "        # best_w2를 현재의 w2로 갱신합니다.\n",
    "        best_b = b\n",
    "        \n",
    "        # 현재의 epoch, w1, w2, error를 print하여 확인합니다.\n",
    "        print(\"epoch = {0:4}, w1 = {1:.5f}, w2 = {2:.5f}, b = {3:.5f}, error = {4:.5f}\".format(epoch, w1, w2, b, error))\n",
    "\n",
    "# best_epoch, best_w1, best_w2, best_error를 print하여 확인합니다.\n",
    "print(\"----\" * 15)\n",
    "print(\"epoch = {0:4}, w1 = {1:.5f}, w2 = {2:.5f}, b = {3:.5f}, error = {4:.5f}\".format(best_epoch, best_w1, best_w2, best_b, best_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Search**의 결과를 보면 error가 이전 Case보다 훨씬 높아진 것을 볼 수 있습니다. 학습할 파라미터(w1, w2, b)가 많아지니까 10000번으로는 좋은 결과를 낼 수 없는 것입니다. 이전 처럼 좋은 결과를 내려면 epoch을 100000번 이상 시도해야 할 것 입니다. **Random Search**는 이렇게 단순한 랜덤이라는 특징 학습할 파라미터가 많아지면 좋은 결과를 얻기위해서는 epoch이 기하급수적으로 늘어나야 한다는, 즉 실행속도가 급격하게 느려진다는 단점이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Idea - h-step Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 번째로 적용해볼 학습 알고리즘은 **h-step Search**입니다. 이 알고리즘은 처음에 랜덤으로 **가중치(w1, w2)와 bias(b)**를 지정한다음 h만큼씩 w1, w2, b를 크거나 작게하고 error를 구한 뒤 이전의 error와 비교하는 것을 반복하며 가장 error가 작은 최적의 w1, w2, b를 찾습니다. 즉, h 간격(step)만큼 이동하며 최적의 w1, w2, b를 찾는 알고리즘입니다. 이 경우 h를 어떤걸로 세팅하냐에 따라서 이동하는 속도가 달라집니다. 즉, h가 너무 크면 속도가 빠르지면 정확하지 않고, h가 너무 작으면 정확하지만 속도가 너무 느립니다. 따라서 어떤 값을 h로 지정할지가 중요한데, 이 h값을 찾는게 어렵습니다. 결론적으로 h값만 잘 정한다면 **random search**보다 빠르게 최적의 w1을 찾을 수 있습니다.\n",
    "\n",
    "`random search`와 마찬가지로 첫 w1, w2, b의 범위를 -1에서 +1로 한정하겠습니다. 범위를 좁혀야 문제를 빨리 풀 수 있기 때문입니다. 또한 모든 알고리즘의 반복 횟수(epoch)는 10000으로 통일하고 오차는 [MAE(Mean Absolute Error)](https://medium.com/@ewuramaminka/mean-absolute-error-mae-sample-calculation-6eed6743838a)로 구하겠습니다.\n",
    "\n",
    "**h-step Search** 알고리즘의 순서는 다음과 같습니다.\n",
    "\n",
    "```\n",
    "1. 몇 번(epoch) h만큼 step을 이동하며 w1, w2, b을 구할지, 즉 몇 번 for문을 돌지 정한다.\n",
    "2. 첫 w, w21을 랜덤으로 지정한다.\n",
    "3. 현재의 w1 * x1 + w2 * x2 + b(y_predict), y와의 오차를 구한다. 이 오차는 현재 오차가 된다.\n",
    "4. w1에 h를 더한 뒤, y와의 오차를 구한다.\n",
    "5. 4에서 구한 오차가 현재 오차보다 더 작다면 w1을 w1+h로 갱신한다.\n",
    "6. 반대로 4에서 구한 오차가 더 크다면 w1에 h를 뺀 뒤, y와의 오차를 구한다.\n",
    "7. 6에서 구한 오차가 현재 오차보다 더 작다면 w1을 w1-h로 갱신한다.\n",
    "8. 4~7번을 w2, b에 대해서 각 각 진행한다.\n",
    "9. 3~8번을 epoch만큼 반복한다. 오차가 일정이하로 내려가면 중단한다.\n",
    "```\n",
    "\n",
    "h값에 따라 오차가 일정 수준 이하로 내려가지 못할 수 있습니다. 따라서 의미없이 알고리즘이 실행되는 것을 막기위해 오차가 특정 수준 이하로 내려가면 알고리즘을 중단합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, w1 = -0.42789, w2 = 0.91621, b = 0.54063, error = 0.41814\n",
      "epoch = 1, w1 = -0.43789, w2 = 0.92621, b = 0.53063, error = 0.40814\n",
      "epoch = 2, w1 = -0.44789, w2 = 0.91621, b = 0.52063, error = 0.39670\n",
      "epoch = 3, w1 = -0.45789, w2 = 0.90621, b = 0.51063, error = 0.38526\n",
      "epoch = 4, w1 = -0.46789, w2 = 0.89621, b = 0.50063, error = 0.37382\n",
      "epoch = 5, w1 = -0.47789, w2 = 0.88621, b = 0.49063, error = 0.36237\n",
      "epoch = 6, w1 = -0.48789, w2 = 0.87621, b = 0.48063, error = 0.35189\n",
      "epoch = 7, w1 = -0.47789, w2 = 0.86621, b = 0.47063, error = 0.34329\n",
      "epoch = 8, w1 = -0.46789, w2 = 0.87621, b = 0.46063, error = 0.33350\n",
      "epoch = 9, w1 = -0.45789, w2 = 0.87621, b = 0.45063, error = 0.32430\n",
      "epoch = 10, w1 = -0.44789, w2 = 0.87621, b = 0.44063, error = 0.31511\n",
      "epoch = 11, w1 = -0.43789, w2 = 0.87621, b = 0.43063, error = 0.30591\n",
      "epoch = 12, w1 = -0.42789, w2 = 0.86621, b = 0.42063, error = 0.29736\n",
      "epoch = 13, w1 = -0.41789, w2 = 0.87621, b = 0.41063, error = 0.28752\n",
      "epoch = 14, w1 = -0.40789, w2 = 0.88621, b = 0.40063, error = 0.27780\n",
      "epoch = 15, w1 = -0.40789, w2 = 0.88621, b = 0.39063, error = 0.26913\n",
      "epoch = 16, w1 = -0.39789, w2 = 0.89621, b = 0.38063, error = 0.25935\n",
      "epoch = 17, w1 = -0.39789, w2 = 0.89621, b = 0.37063, error = 0.25073\n",
      "epoch = 18, w1 = -0.38789, w2 = 0.90621, b = 0.36063, error = 0.24094\n",
      "epoch = 19, w1 = -0.38789, w2 = 0.90621, b = 0.35063, error = 0.23234\n",
      "epoch = 20, w1 = -0.37789, w2 = 0.91621, b = 0.34063, error = 0.22255\n",
      "epoch = 21, w1 = -0.36789, w2 = 0.91621, b = 0.33063, error = 0.21335\n",
      "epoch = 22, w1 = -0.35789, w2 = 0.91621, b = 0.32063, error = 0.20415\n",
      "epoch = 23, w1 = -0.34789, w2 = 0.91621, b = 0.31063, error = 0.19496\n",
      "epoch = 24, w1 = -0.33789, w2 = 0.91621, b = 0.30063, error = 0.18576\n",
      "epoch = 25, w1 = -0.32789, w2 = 0.91621, b = 0.29063, error = 0.17656\n",
      "epoch = 26, w1 = -0.31789, w2 = 0.90621, b = 0.28063, error = 0.16849\n",
      "epoch = 27, w1 = -0.30789, w2 = 0.91621, b = 0.27063, error = 0.15817\n",
      "epoch = 28, w1 = -0.29789, w2 = 0.91621, b = 0.26063, error = 0.14902\n",
      "epoch = 29, w1 = -0.28789, w2 = 0.92621, b = 0.25063, error = 0.13920\n",
      "epoch = 30, w1 = -0.28789, w2 = 0.92621, b = 0.24063, error = 0.13076\n",
      "epoch = 31, w1 = -0.27789, w2 = 0.93621, b = 0.23063, error = 0.12079\n",
      "epoch = 32, w1 = -0.27789, w2 = 0.93621, b = 0.22063, error = 0.11257\n",
      "epoch = 33, w1 = -0.26789, w2 = 0.94621, b = 0.21063, error = 0.10240\n",
      "epoch = 34, w1 = -0.26789, w2 = 0.94621, b = 0.20063, error = 0.09448\n",
      "epoch = 35, w1 = -0.25789, w2 = 0.95621, b = 0.19063, error = 0.08400\n",
      "epoch = 36, w1 = -0.25789, w2 = 0.95621, b = 0.18063, error = 0.07644\n",
      "epoch = 37, w1 = -0.24789, w2 = 0.96621, b = 0.17063, error = 0.06573\n",
      "epoch = 38, w1 = -0.23789, w2 = 0.96621, b = 0.16063, error = 0.05665\n",
      "epoch = 39, w1 = -0.22789, w2 = 0.96621, b = 0.15063, error = 0.04761\n",
      "epoch = 40, w1 = -0.21789, w2 = 0.96621, b = 0.14063, error = 0.03861\n",
      "epoch = 41, w1 = -0.20789, w2 = 0.97621, b = 0.13063, error = 0.02834\n",
      "epoch = 42, w1 = -0.20789, w2 = 0.97621, b = 0.12063, error = 0.02089\n",
      "epoch = 43, w1 = -0.19789, w2 = 0.98621, b = 0.11063, error = 0.00984\n",
      "epoch = 44, w1 = -0.19789, w2 = 0.98621, b = 0.10063, error = 0.00567\n",
      "epoch = 45, w1 = -0.18789, w2 = 0.99621, b = 0.10063, error = 0.00413\n",
      "------------------------------------------------------------\n",
      "epoch = 45, w1 = -0.18789, w2 = 0.99621, b = 0.10063, error = 0.00413\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행할 때마다 결과가 달라지지 않게 하기위하여 seed를 고정합니다.\n",
    "# seed는 임의로 37로 고정하겠습니다.\n",
    "np.random.seed(31)\n",
    "\n",
    "# h만큼 이동하는 횟수인 epoch을 10000으로 지정합니다.\n",
    "num_epoch = 10000\n",
    "\n",
    "# -1 ~ +1 사이의 값중 랜덤으로 w1을 지정(초기화)합니다.\n",
    "w1 = np.random.uniform(low=-1.0, high=1.0)\n",
    "# -1 ~ +1 사이의 값중 랜덤으로 w2을 지정(초기화)합니다.\n",
    "w2 = np.random.uniform(low=-1.0, high=1.0)\n",
    "# -1 ~ +1 사이의 값중 랜덤으로 b를 지정(초기화)합니다.\n",
    "b = np.random.uniform(low=-1.0, high=1.0)\n",
    "\n",
    "# 이동하는 간격(step)인 h를 0.01로 지정합니다.\n",
    "# w1, w2, b가 -1 ~ +1사이의 값이기 때문에 적당히 작은 값으로 지정하였습니다.\n",
    "h = 0.01\n",
    "\n",
    "# epoch만큼 반복되는 for문을 선언합니다.\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    # x1과 w1의 곱과 x2와 w2의 곱, b를 더한 것을 y_predict 변수에 저장합니다.\n",
    "    # y_predict는 y를 예측한 값을 뜻합니다.\n",
    "    y_predict = w1 * x1 + w2 * x2 + b\n",
    "    # 현재 오차를 구하여 current_error에 저장합니다.\n",
    "    # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "    current_error = np.mean(np.abs(y_predict - y))\n",
    "    \n",
    "    # 현재의 epoch, w1, current_error를 print하여 확인합니다.\n",
    "    print(\"epoch = {0}, w1 = {1:.5f}, w2 = {2:.5f}, b = {3:.5f}, error = {4:.5f}\".format(epoch, w1, w2, b, current_error))\n",
    "    \n",
    "    # 만약 현재 오차가 0.005보다 작다면 for문을 마치고 알고리즘을 끝냅니다.\n",
    "    # h값에 따라 오차가 일정 이하로 내려가지 못하는 경우가 생깁니다.\n",
    "    # 이 경우 for문이 의미없이 반복되는 것을 막기 위한 코드입니다.\n",
    "    # 0.005를 0.0001과 같이 더 작은 값으로 한다면 for문이 의미없이 반복되는 것을 확인할 수 있습니다.\n",
    "    if current_error < 0.005:\n",
    "        # break는 for문을 바로 마칩니다.\n",
    "        break\n",
    "\n",
    "    # h만큼 w1가 커지는 방향으로 이동합니다. 즉, w1에 h를 더하여 y_predict를 구합니다.\n",
    "    y_predict = (w1 + h) * x1 + w2 * x2 + b\n",
    "    # 이 때의 오차를 구한 뒤 h_plus_error에 저장합니다.\n",
    "    # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "    h_plus_error = np.mean(np.abs(y_predict - y))\n",
    "    \n",
    "    # 현재 오차인 current_error와 h만큼 이동했을 때의 오차인 h_plus_error를 비교합니다.\n",
    "    # h_plus_error가 더 작다면 w1을 w1+h로 갱신합니다.\n",
    "    if h_plus_error < current_error:\n",
    "        w1 = w1 + h\n",
    "        \n",
    "    # h_plus_error가 더 크다면 이번엔 h만큼 w가 작아지는 방향으로 이동합니다. 즉, w1에 h를 빼고 y_predict를 구합니다.   \n",
    "    else:\n",
    "        # w1에 h를 빼고 y_predict를 구합니다. \n",
    "        y_predict = (w1 - h) * x1 + w2 * x2 + b\n",
    "        # 이 때의 오차를 구한 뒤 h_minus_error에 저장합니다.\n",
    "        # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "        h_minus_error = np.mean(np.abs(y_predict - y))\n",
    "        \n",
    "        # h_plus_error가 더 작다면 w1을 w1-h로 갱신합니다.\n",
    "        if h_minus_error < current_error:\n",
    "            w1 = w1 - h\n",
    "            \n",
    "    # h만큼 w2가 커지는 방향으로 이동합니다. 즉, w2에 h를 더하여 y_predict를 구합니다.\n",
    "    y_predict = w1 * x1 + (w2 + h) * x2 + b\n",
    "    # 이 때의 오차를 구한 뒤 h_plus_error에 저장합니다.\n",
    "    # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "    h_plus_error = np.mean(np.abs(y_predict - y))\n",
    "    \n",
    "    # 현재 오차인 current_error와 h만큼 이동했을 때의 오차인 h_plus_error를 비교합니다.\n",
    "    # h_plus_error가 더 작다면 w2을 w2+h로 갱신합니다.\n",
    "    if h_plus_error < current_error:\n",
    "        w2 = w2 + h\n",
    "        \n",
    "    # h_plus_error가 더 크다면 이번엔 h만큼 w가 작아지는 방향으로 이동합니다. 즉, w2에 h를 빼고 y_predict를 구합니다.   \n",
    "    else:\n",
    "        # w2에 h를 빼고 y_predict를 구합니다. \n",
    "        y_predict = w1 * x1 + (w2 - h) * x2 + b\n",
    "        # 이 때의 오차를 구한 뒤 h_minus_error에 저장합니다.\n",
    "        # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "        h_minus_error = np.mean(np.abs(y_predict - y))\n",
    "        \n",
    "        # h_plus_error가 더 작다면 w2을 w2-h로 갱신합니다.\n",
    "        if h_minus_error < current_error:\n",
    "            w2 = w2 - h\n",
    "            \n",
    "    # h만큼 b가 커지는 방향으로 이동합니다. 즉, b에 h를 더하여 y_predict를 구합니다.\n",
    "    y_predict = w1 * x1 + w2 * x2 + (b + h)\n",
    "    # 이 때의 오차를 구한 뒤 h_plus_error에 저장합니다.\n",
    "    # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "    h_plus_error = np.mean(np.abs(y_predict - y))\n",
    "    \n",
    "    # 현재 오차인 current_error와 h만큼 이동했을 때의 오차인 h_plus_error를 비교합니다.\n",
    "    # h_plus_error가 더 작다면 b을 b+h로 갱신합니다.\n",
    "    if h_plus_error < current_error:\n",
    "        b = b + h\n",
    "        \n",
    "    # h_plus_error가 더 크다면 이번엔 h만큼 b가 작아지는 방향으로 이동합니다. 즉, b에 h를 빼고 y_predict를 구합니다.   \n",
    "    else:\n",
    "        # b에 h를 빼고 y_predict를 구합니다. \n",
    "        y_predict = w1 * x1 + w2 * x2 + (b - h)\n",
    "        # 이 때의 오차를 구한 뒤 h_minus_error에 저장합니다.\n",
    "        # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "        h_minus_error = np.mean(np.abs(y_predict - y))\n",
    "        \n",
    "        # h_plus_error가 더 작다면 b을 b-h로 갱신합니다.\n",
    "        if h_minus_error < current_error:\n",
    "            b = b - h\n",
    "            \n",
    "# 최종 결과를 확인합니다. \n",
    "# epoch, w1, w2, b, current_error를 출력합니다.\n",
    "print(\"----\" * 15)   \n",
    "print(\"epoch = {0}, w1 = {1:.5f}, w2 = {2:.5f}, b = {3:.5f}, error = {4:.5f}\".format(epoch, w1, w2, b, current_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Search**에서 달성한 오차보다 작은 오차를 더 적은 실행만에 도달한 것을 확인할 수 있습니다. 즉, **h-step Search**는 **Random Search**보다 효율적으로 학습하며 학습할 파라미터가 많을 수록 성능이 더 좋다는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Idea - Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세 번째로 적용해볼 학습 알고리즘은 **Gradient Descent**입니다. 이 알고리즘은 PPT로 더 자세히 알아볼 것이므로 지금은 간단하게 알아보겠습니다. **Gradient Descent**는 **h-step Search**와 기본적으로 처음에 랜덤으로 가중치(w1, w2), bias(b)를 지정한다음 w1, w2, b을 조금씩 이동하며(값을 바꾸며) error을 줄여나간다는 점에서 동일합니다. 다만 이동하는 방법에서 차이가 있습니다. **Gradient Descent**는 현재 정답(y)과 예측값(y_predict)와의 차이를 고려하여 w1, w2, b을 이동한다고 생각하면 됩니다. 가령 feature(x1, x2)의 값들이 양수(+)라고 가정할 때, y_predict가 y보다 클수록 w1, w2, b을 작게 하여 y_predict를 감소시키고 y_predict가 y보다 작을수록 w1, w2, b을 크게 하여 y_predict를 증가시킨다고 생각하면 됩니다. 더 정확하게는 수학적으로 기울기 또는 변화율을 의미하는 Gradient와 Cost Function(비용함수)를 고려하여 w, b의 이동방향과 간격을 정하는 알고리짐이지만 이는 PPT에서 다루도록 하겠습니다.\n",
    "\n",
    "**Gradient Descent**의 장점은 h값을 스스로 조절하기 때문에 **h-step Search**보다 정답에 빠르게 도달할 수 있다는 것입니다. 따라서 딥러닝에에서 학습 알고리즘으로 가장 많이 응용되어 사용되고 있는 알고리즘입니다.\n",
    "\n",
    "<img src = \"https://drive.google.com/uc?export=view&id=1Du4wnbtt9d8v86FZwZf_fJC7dxUGVbyY\" height = \"600\" width = \"500\" />\n",
    "\n",
    "그림으로 **Gradient Descent**를 간단하게 보자면 위와 같이 gradient(기울어진 정도)를 따라서 w, b를 이동하며 cost(error)를 감소시킵니다.\n",
    "\n",
    "\n",
    "\n",
    "**h-step Search**와 마찬가지로 첫 w1, w2, b의 범위를 -1에서 +1로 한정하겠습니다. 범위를 좁혀야 문제를 빨리 풀 수 있기 때문입니다. 또한 모든 알고리즘의 반복 횟수(epoch)는 10000으로 통일하고 오차는 [MAE(Mean Absolute Error)](https://medium.com/@ewuramaminka/mean-absolute-error-mae-sample-calculation-6eed6743838a)로 구하겠습니다.\n",
    "\n",
    "Gradient Descent 알고리즘의 순서는 다음과 같습니다.\n",
    "\n",
    "```\n",
    "1. 몇 번(epoch) 이동하며 w1, w2, b을 구할지, 즉 몇 번 for문을 돌지 정한다.\n",
    "2. 첫 w1, w2, b를 랜덤으로 지정한다.\n",
    "3. 현재의 w1에 x1을 곱한 것과 w2와 x2를 곱한 것, b를 더한 값(y_predict)와, y와의 오차를 구한다. 이 오차는 현재 오차가 된다.\n",
    "4. w1, w2, b을 이동한다. 즉, w1을 w1- np.mean((y_predict - y) * x1))로, w2을 w2- np.mean((y_predict - y) * x2))로, b는 b - np.mean(y_predict - y)로 갱신한다.\n",
    "5. 3~5번을 epoch만큼 반복한다. 오차가 일정이하로 내려가면 중단한다.\n",
    "```\n",
    "\n",
    "**Gradient Descent** 역시 **h-step Search**와 같이 오차가 일정 수준 이하로 내려가지 못할 수 있습니다. 따라서 의미없이 알고리즘이 실행되는 것을 막기위해 오차가 특정 수준 이하로 내려가면 알고리즘을 중단합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, w1 = 0.93488, w2 = -0.02592, b = 0.09876, error = 0.71317\n",
      "------------------------------------------------------------\n",
      "epoch = 4, w1 = 0.88300, w2 = -0.07779, b = 0.09899, error = 0.00254\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행할 때마다 결과가 달라지지 않게 하기위하여 seed를 고정합니다.\n",
    "# seed는 임의로 37로 고정하겠습니다.\n",
    "np.random.seed(37)\n",
    "\n",
    "# h만큼 이동하는 횟수인 epoch을 10000으로 지정합니다.\n",
    "num_epoch = 10000\n",
    "\n",
    "# -1 ~ +1 사이의 값중 랜덤으로 w1을 지정(초기화)합니다.\n",
    "w1 = np.random.uniform(low=-1.0, high=1.0)\n",
    "# -1 ~ +1 사이의 값중 랜덤으로 w2을 지정(초기화)합니다.\n",
    "w2 = np.random.uniform(low=-1.0, high=1.0)\n",
    "# -1 ~ +1 사이의 값중 랜덤으로 b을 지정(초기화)합니다.\n",
    "b = np.random.uniform(low=-1.0, high=1.0)\n",
    "\n",
    "# epoch만큼 반복되는 for문을 선언합니다.\n",
    "for epoch in range(num_epoch):\n",
    " \n",
    "    # x1과 w1의 곱과 x2와 w2의 곱, b를 더한 것을 y_predict 변수에 저장합니다.\n",
    "    # y_predict는 y를 예측한 값을 뜻합니다.\n",
    "    y_predict = w1 * x1 + w2 * x2 + b\n",
    "    \n",
    "    # 현재 오차를 구하여 error에 저장합니다.\n",
    "    # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "    error = np.mean(np.abs(y_predict - y))\n",
    "    \n",
    "    \n",
    "    # 만약 현재 오차가 0.005보다 작다면 for문을 마치고 알고리즘을 끝냅니다.\n",
    "    # h값에 따라 오차가 일정 이하로 내려가지 못하는 경우가 생깁니다.\n",
    "    # 이 경우 for문이 의미없이 반복되는 것을 막기 위한 코드입니다.\n",
    "    # 0.005를 0.0001과 같이 더 작은 값으로 한다면 for문이 의미없이 반복되는 것을 확인할 수 있습니다.    \n",
    "    if error < 0.005:\n",
    "        # break는 for문을 바로 마칩니다.\n",
    "        break\n",
    "\n",
    "    # w1을 w1- np.mean((y_predict - y) * x1))로 갱신합니다.\n",
    "    w1 = w1 - np.mean(((y_predict - y) * x1))\n",
    "    # w2을 w2- np.mean((y_predict - y) * x2))로 갱신합니다.\n",
    "    w2 = w2 - np.mean(((y_predict - y) * x2))\n",
    "    # w2을 w2- np.mean(y_predict - y))로 갱신합니다.\n",
    "    b = b - np.mean((y_predict - y))\n",
    "    \n",
    "    # 10회마다 epoch, w1, w2, b, error를 출력하여 확인합니다.\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"epoch = {0}, w1 = {1:.5f}, w2 = {2:.5f}, b = {3:.5f}, error = {4:.5f}\".format(epoch, w1, w2, b, error))\n",
    "    \n",
    "    \n",
    "# 최종 결과를 확인합니다. \n",
    "# epoch, w1, w2, b, current_error를 출력합니다.\n",
    "print(\"----\" * 15)\n",
    "print(\"epoch = {0}, w1 = {1:.5f}, w2 = {2:.5f}, b = {3:.5f}, error = {4:.5f}\".format(epoch, w1, w2, b, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**알고리즘은 **h-step Search**보다 작은 오차를 더욱 빠르게 달성한 것을 볼 수 있습니다. 즉, **Gradient Descent**는 **h-step Search**보다 성능이 좋은 알고리즘이란 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 Learning Rate에 대해 알아보고 적용해보겠습니다. **Gradient Descent** 알고리즘도 **h-step Search**처럼 이동하는 간격(h)를 지정할 수 있습니다. w를 이동할 때 이동하는 간격에 **Learning Rate**를 곱해주어 이동하는 간격을 늘리거나 줄일수 있습니다. h와 마찬가지로 **Learning Rate**를 어떤걸로 세팅하냐에 따라서 이동하는 속도가 달라집니다. 즉, **Learning Rate**가 너무 크면 속도가 빠르지면 정확하지 않고, 반대로 너무 작으면 정확하지만 속도가 너무 느립니다. 따라서 어떤 값을 **Learning Rate**로 지정할지가 중요한데, 이 **Learning Rate**값을 찾는게 어렵습니다. 결론적으로 **Learning Rate**값만 잘 정한다면 보다 빠르게 최적의 w를 찾을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, w1 = 0.93153, w2 = -0.02926, b = 0.06008, error = 0.61317\n",
      "------------------------------------------------------------\n",
      "epoch = 3, w1 = 0.88414, w2 = -0.07666, b = -0.00008, error = 0.00365\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행할 때마다 결과가 달라지지 않게 하기위하여 seed를 고정합니다.\n",
    "# seed는 임의로 37로 고정하겠습니다.\n",
    "np.random.seed(37)\n",
    "\n",
    "# h만큼 이동하는 횟수인 epoch을 10000으로 지정합니다.\n",
    "num_epoch = 10000\n",
    "\n",
    "# learning_rate를 1.1로 지정합니다. 가중치 이동폭을 조금 늘린 것입니다.\n",
    "learning_rate = 1.1\n",
    "\n",
    "# -1 ~ +1 사이의 값중 랜덤으로 w1을 지정(초기화)합니다.\n",
    "w1 = np.random.uniform(low=-1.0, high=1.0)\n",
    "# -1 ~ +1 사이의 값중 랜덤으로 w2을 지정(초기화)합니다.\n",
    "w2 = np.random.uniform(low=-1.0, high=1.0)\n",
    "# -1 ~ +1 사이의 값중 랜덤으로 b을 지정(초기화)합니다.\n",
    "b = np.random.uniform(low=-1.0, high=1.0)\n",
    "\n",
    "# epoch만큼 반복되는 for문을 선언합니다.\n",
    "for epoch in range(num_epoch):\n",
    " \n",
    "    # x1과 w1의 곱과 x2와 w2의 곱, b를 더한 것을 y_predict 변수에 저장합니다.\n",
    "    # y_predict는 y를 예측한 값을 뜻합니다.\n",
    "    y_predict = w1 * x1 + w2 * x2 + b\n",
    "    \n",
    "    # 현재 오차를 구하여 error에 저장합니다.\n",
    "    # 오차는 MAE(Mean Absolute Error)를 사용합니다.\n",
    "    error = np.mean(np.abs(y_predict - y))\n",
    "    \n",
    "    \n",
    "    # 만약 현재 오차가 0.005보다 작다면 for문을 마치고 알고리즘을 끝냅니다.\n",
    "    # h값에 따라 오차가 일정 이하로 내려가지 못하는 경우가 생깁니다.\n",
    "    # 이 경우 for문이 의미없이 반복되는 것을 막기 위한 코드입니다.\n",
    "    # 0.005를 0.0001과 같이 더 작은 값으로 한다면 for문이 의미없이 반복되는 것을 확인할 수 있습니다.    \n",
    "    if error < 0.005:\n",
    "        # break는 for문을 바로 마칩니다.\n",
    "        break\n",
    "\n",
    "    # w1을 w1- np.mean((y_predict - y) * x1))로 갱신합니다.\n",
    "    w1 = w1 - learning_rate * np.mean(((y_predict - y) * x1))\n",
    "    # w2을 w2- np.mean((y_predict - y) * x2))로 갱신합니다.\n",
    "    w2 = w2 - learning_rate * np.mean(((y_predict - y) * x2))\n",
    "    # w2을 w2- np.mean(y_predict - y))로 갱신합니다.\n",
    "    b = b - learning_rate * np.mean((y_predict - y))\n",
    "    \n",
    "    # 10회마다 epoch, w1, w2, error를 출력하여 확인합니다.\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"epoch = {0}, w1 = {1:.5f}, w2 = {2:.5f}, b = {3:.5f}, error = {4:.5f}\".format(epoch, w1, w2, b, error))\n",
    "    \n",
    "    \n",
    "# 최종 결과를 확인합니다. \n",
    "# epoch, w1, w2, current_error를 출력합니다.\n",
    "print(\"----\" * 15)\n",
    "print(\"epoch = {0}, w1 = {1:.5f}, w2 = {2:.5f}, b = {3:.5f}, error = {4:.5f}\".format(epoch, w1, w2, b, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate**를 1보다 크게 할 경우 보다 빠르게 0.005이하의 오차를 달성한 것을 확인할 수 있습니다. 이는 이동하는 보폭이 크기 때문입니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
